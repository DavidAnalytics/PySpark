# -*- coding: utf-8 -*-
"""GlobalSalesRecord.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SGdKaKv0LOFoJWThvEnk1y3vZWTsa-C8
"""

!apt-get update
!apt-get install openjdk-11-jdk -y
!pip install pyspark

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/usr/local/lib/python3.11/dist-packages/pyspark"

"""First thing, we need to get or create a spark session

The Spark Session is the entry point for high-level Spark functionality. It is initiated using the Sparksession.builder
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import DataFrame
from pyspark.sql.types import DateType

spk = SparkSession.builder.master("local[*]").appName('globalSales').getOrCreate()

print(spk.version)



"""Then we read in the csv file using the spark.read.csv"""

global_df = spk.read.csv("global_sales_records.csv", header=True,inferSchema=True)

global_df.show(5)

global_df.printSchema()

# Rename the columns, to remove the spaces
def rename_columns_remove_spaces(df: DataFrame) -> DataFrame:
    """Renames columns in a PySpark DataFrame to replace spaces with underscores."""
    for col_name in df.columns:
        new_col_name = col_name.replace(" ", "_")
        df = df.withColumnRenamed(col_name, new_col_name)
    return df

global_df = rename_columns_remove_spaces(global_df)

global_df.show(50)

# Data Transformation
# Transform the Order and Ship Date

global_df = global_df.withColumn("Order_Date", F.to_date(F.col("Order_Date"), "M/d/yyy"))

# Assignment Transform the Ship Date

global_df = global_df.withColumn("Ship_Date", F.to_date(F.col("Ship_Date"), "M/d/yyy"))

"""## Top Performing Regions By Revenue"""

top_region_df = global_df.groupBy("Region").agg(F.sum("Total_Revenue").alias("Revenue_Generated"))

# Sort by total revenue in descending order and take the top 1 region
top_region = top_region_df.orderBy(F.col("Revenue_Generated").asc()).limit(3)

top_region = top_region.withColumn("Revenue_Generated", F.format_number(F.col("Revenue_Generated"), 2))

top_region.show()

# Group by 'region' and calculate the average revenue for each region
average_revenue_df = global_df.groupBy("Region").agg(F.avg("Total_Revenue").alias("average_revenue"))

# Sort by average revenue in descending order
top_avg_region = average_revenue_df.orderBy(F.col("average_revenue").desc()).limit(3)

top_avg_region.show()

"""## Underperfroming Countries By Total_Profit"""

# Define a threshold for underperforming
profit_threshold = 50000

# Filter countries with profits below the threshold
underperforming_countries_df = global_df.filter(F.col("Total_Profit") < profit_threshold)

# Sort by Total_Profit in ascending order (lowest first)
underperforming_countries_df = underperforming_countries_df.orderBy(F.col("Total_Profit").asc())

underperforming_countries_df.show(5)

"""## Sales Channel Revenue Comparison (Online vs OPffline)"""

# Group by 'Sales_Channel' and sum the 'Revenue' for each channel
sales_channel_revenue_df = global_df.groupBy("Sales_Channel").agg(F.sum("Total_Revenue").alias("Revenue_Generated"))

# Format the 'Revenue_Generated' to 2 decimal places
sales_channel_revenue_df = sales_channel_revenue_df.withColumn("Revenue_Generated", F.format_number(F.col("Revenue_Generated"), 2))

sales_channel_revenue_df.show()